{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to my homelab!","text":"<p>This is where I will be documenting my homelab projects.</p> <p>Right now, I only have one project in the works. You can read about it here:</p>"},{"location":"#3-node-proxmox-cluster","title":"3-Node Proxmox Cluster","text":"<p>I also maintain a journal for any updates I make to my lab, including new project planning. You can find this here:</p>"},{"location":"#my-journal","title":"My Journal","text":""},{"location":"journal/","title":"Journal","text":""},{"location":"journal/2026/01/03/happy-late-new-years/","title":"Happy (late) New Years!","text":"<p>Here's a quick-ish progress report on the goals I have set over a week ago:</p> <ul> <li>I have installed and configured fail2ban for both ssh and web servers on each node.</li> <li>I have (hopefully) solved my network issues with a new approach (see below).</li> <li>I have replaced <code>cloudflared</code> with <code>tailscale</code> ! </li> </ul>"},{"location":"journal/2026/01/03/happy-late-new-years/#network-changes","title":"Network Changes","text":"<p>I have also made a few changes to my network structure. I have implemented <code>VXLAN</code> to connect my VMs and Containers via a new <code>10.0.100.0/24</code> network. However for ease of use, I wanted to also use DHCP for less critical applications, and I also wanted to be able to route out to the internet over that same network. </p> <p>Currently, Proxmox 9 does not support this for VXLAN as it does for the Simple zone, and Simple zones themselves do not provide layer-2 benefits either, so a custom solution must be created.</p> <p>I got this to work by creating a simple zone with DHCP to act as the host-facing interface that each VM will interact with, effectively wrapping over the VXLAN zone in a (hopefully) clean way. I also made sure to adjust the <code>MTU</code> value down to <code>1450</code> in the simple zone to match what VXLAN needs for overhead. Credit to Mhdo!</p> <p>Below is a custom SDN config (named \"sdn-custom\" to load after the Proxmox managed one) file that helps accomplish this, where <code>vnet1</code> is the Simple zone VNet and <code>vxnet1</code> is for the VXLAN network:</p> <pre><code>auto head\niface head inet manual\n        link-type veth\n        veth-peer-name tail\n\nauto tail\niface tail inet manual\n        link-type veth\n        veth-peer-name head\n\niface vnet1 inet manual\n        bridge_ports head\n\niface vxnet1 inet manual\n        bridge_ports tail\n</code></pre> <p>This configuration successfully assigns IPs to each VM and container I spin up across each node in my cluster, however I still need to get internet connectivity. I initially was looking into bonding each wireless interface across each node, but I still cant find any information online that emulates this, let alone if it is even possible.</p> <p>To keep things (more or less) simple, I have overwritten the SDN interface for <code>vnet1</code> at the top of my <code>sdn-custom</code> config file. each node creates a network bridge <code>vnet1</code> with MASQUERADE pointed towards the wireless interface with default gateway of <code>10.0.100.254/24</code>:</p> <pre><code>auto vnet1\niface vnet1\n        address 10.0.100.254/24\n        bridge_ports none\n        bridge_stp off\n        bridge_fd 0\n\n        post-up   echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n        post-up   iptables -t nat -A POSTROUTING -s '10.0.100.0/24' -o wlp2s0 -j MASQUERADE\n        post-down iptables -t nat -D POSTROUTING -s '10.0.100.0/24' -o wlp2s0 -j MASQUERADE\n        post-up   iptables -t raw -I PREROUTING -i fwbr+ -j CT --zone 1\n        post-down iptables -t raw -D PREROUTING -i fwbr+ -j CT --zone 1\n\n        mtu 1450\n        ip-forward on\n...\n</code></pre> <p>This should overwrite any changes made to <code>vnet1</code>, but I have yet to fully test this. If my connectivity persists across a node's IP getting renewed by the router, then I managed to fix this lol. </p> <p>Through this headache I discovered a weird limitation with Proxmox's UI while getting this all to work. Enabling <code>SNAT</code> in my Simple Zone's subnet will correctly use my <code>wlp2s0</code> interface but unfortunately sets a static IP (I am relying on a dynamic IP from my router). Ideally, I would like to manually set the interface at the very least, along with the ability to enable MASQUERADE instead. Better DNAT/SNAT settings seem to be planned for a future release, which will be good to keep note of for updating later.</p> <p>I still feel like this is messy. Maybe I should be using EVPN instead for this? I really should invest in a dedicated OPNsense router so I can ditch the on-board wireless for 2.5 Gigabit NICs!</p>"},{"location":"journal/2026/01/14/pve-cluster---first-dns-tests/","title":"PVE Cluster - First DNS Tests","text":"<p>A have good news: The network seems to be stable! While Proxmox's SDN still seems to have that bug (DHCP server leases IPs even to statically-assigned guests), so long as I update the IP in the 'IPAM' settings everything continues to work.</p> <p>While I had more important things to work on (firewall, tailnet ACLs, etc.) I decided to test out a split-horizon local DNS within my tailscale network:</p>"},{"location":"journal/2026/01/14/pve-cluster---first-dns-tests/#my-dns-solution","title":"My DNS Solution","text":"<p>This mostly started out of curiosity when I discovered the split-horizon DNS setting under my tailnet's configuration dashboard. I wanted to quickly set up a locally-controlled (not public yet) domain name to assign aliases to the various services I plan on running. I wanted both a DNS server for the actual domain resolution and a reverse proxy for managing the various http/s services / backends I want to access.</p> <p>For this, I chose <code>Technitium</code> for my DNS solution and <code>Zoraxy</code> for my reverse proxy. Each have an easy to use web dashboard and premade LXC install scripts.</p>"},{"location":"journal/2026/02/06/fresh-new-layout/","title":"Fresh New Layout!","text":"<p>Part of a Professional Development activity I am taking on this month is to reformat all of my homelab documentation to to be more formal. Additionally, I plan on adopting a suitable Project Management framework to help prioritize any new features or changes in the future. </p>"},{"location":"journal/2026/02/06/fresh-new-layout/#new-formatting","title":"New formatting","text":"<p>I decided it was worth the effort to convert my markdown documentation over to something more professional and much easier to navigate: <code>MkDocs</code>!</p>"},{"location":"journal/2026/02/24/homelab-nas-project-planning/","title":"Homelab NAS Project - Planning","text":"<p>As my current Homelab consists of only compute so far, I have been thinking about building out some networked storage. One of the two main reasons behind this new requirement is due to the lack of secondary storage for each of my nodes, meaning that my cluster isn't actually High-Availability yet!</p>"},{"location":"journal/2026/02/24/homelab-nas-project-planning/#requirements","title":"Requirements","text":"<p>I have two constraints I need to follow: The NAS must be power-efficient (&lt;50 Watts) and Affordable (less than $200 total). Ideally, I would also like my NAS solution to be compact, taking up no more space than what my current Proxmox cluster does. The NAS must also support a protocol that Proxmox can use for High-Availability, such as iSCSI.</p> <p>After some initial research, these constraints rule out any off-the-shelf retail product, leaving the used market and/or a custom build.</p> <p>Right now, my plan is to search places like eBay for used compact computers and NAS products for good opportunities.</p>"},{"location":"journal/2025/11/23/pve-cluster---nat-w-dhcp-issues/","title":"PVE NAT w/ DHCP Issues","text":"<p>I am currently implementing a private internal subnet that will get connected to the wireless interface via NAT by following parts of this guide.</p> <p>However, I am currently running into issues with getting the internal DHCP server up and running, could be something to do with it starting up before the other interfaces have time to catch up, might try to add some artificial wait time for the service on startup?</p> <p>My plan is to experiment and break things up until Thanksgiving, where I will have a bit of free time to re-provision everything and come up with a better plan.</p>"},{"location":"journal/2025/12/01/pve-cluster---2nd-attempt/","title":"PVE Cluster - 2nd Attempt","text":"<p>Over thanksgiving break, I have taken what I have learned form my initial test of running a <code>Proxmox</code> cluster and have reinstalled everything from scratch.</p>"},{"location":"journal/2025/12/01/pve-cluster---2nd-attempt/#for-each-machine","title":"For each machine:","text":"<ul> <li>I set up a network for for internal ssh and <code>corosync</code> cluster traffic on <code>10.0.1.1-3/24</code> using the gigabit ethernet interface on each board.</li> <li>Secured the ssh server to only accept keys instead of passwords (for now, only the default root user is set up, the plan is to move away from the root user in favor for a less privileged account with SUDO access)</li> <li>Enabled the Wireless interfaces on the board, this time following a forum post that implements Source Network Address Translation (SNAT).<ul> <li>I believe this is set up to use Software Defined Networking across all three nodes, meaning that the internal SNAT network is shared and accessible between each node. Further testing and research is required.</li> </ul> </li> <li>Updated and configured each host via the Proxmox Post-Install Script.</li> </ul>"},{"location":"journal/2025/12/01/pve-cluster---2nd-attempt/#current-plans-in-the-works","title":"Current plans in the works:","text":"<ul> <li>Securing the network: Enable firewall, create security groups for web console and ssh access.</li> <li>Securing the hardware: Create non-root administrative users, access root privileges with SUDO.</li> <li>Looking into creating a bonded network for the wireless interfaces?</li> <li>Install some containers, test out network reliability.</li> </ul>"},{"location":"journal/2025/12/24/pve-cluster---lab-planning/","title":"PVE Cluster - Lab Planning","text":"<p>Now that finals is over, I've had a bit of time to work on my lab cluster again!</p>"},{"location":"journal/2025/12/24/pve-cluster---lab-planning/#my-goals-for-winter-break-are-as-follows","title":"My goals for winter break are as follows:","text":"<ul> <li> <p>Secure the Software / OS</p> <ul> <li>Create non-root admin accounts w/ sudo access</li> <li>Find a proxmox hardening guide somewhere?</li> </ul> </li> <li> <p>Secure the Network</p> <ul> <li>Enable firewall (enable web portal and ssh only over the wired interface?)</li> <li>Install fail2ban (web portal and ssh) to block bot traffic, low-effort attacks</li> <li>Find a network traffic monitoring tool?</li> </ul> </li> <li> <p>Deploy a few test services (various VMs, Containers)</p> <ul> <li>Ensure network connectivity for instances!</li> </ul> </li> <li> <p>Deploy a HA container for <code>cloudflard</code> network tunnel</p> <ul> <li>For now, this seems to be a secure way to access the network from the outside. I would like to use a reverse proxy in the future but will require further research.</li> </ul> </li> </ul>"},{"location":"journal/2025/12/24/pve-cluster---lab-planning/#happy-holidays","title":"Happy Holidays!","text":""},{"location":"proxmox-cluster/","title":"3-Node Proxmox Cluster","text":""},{"location":"proxmox-cluster/#project-overview","title":"Project Overview","text":"<p>My goal for this project is to build a Cluster of <code>Proxmox Hypervisors</code> that will allow me to run future projects on top of. This project will allow me to get my hands dirty with virtualization and expand on my computer networking skillset through a secure homelab network allowing me to host a variety of services for myself.</p> <p>For hardware, I limited myself to only spend $100, and I was able to find a good deal on 3 <code>Lenovo ThinkCenter M73</code> machines, each with an <code>Intel Core i3-4130T</code> @ 2.9GHz with 8GB of RAM and a 256GB SATA SSD. While the i3 processors are limiting, I plan on upgrading them (perhaps with <code>i5-4590Ts</code>) when the need arises. </p>"},{"location":"proxmox-cluster/#limitations","title":"Limitations","text":"<p>At my current residence, I do not have control over the network settings, including assigning static IPs or port-forwarding, and am limited to a wireless connection. My temporary solution is to use a direct connection via a statically-assigned private subnet operating over a switch, the same one that will be used for each Proxmox node to talk to each other.</p> <p>This of course leaves the WAN to deal with. There are really only two other ways for me to get each node online, either by creating a network bridge on my desktop and using a Router with NAT, or enable the wireless NICs on each of my nodes. I initially attempted the first option, and while it worked temporarily I found that I had issues with getting a reliable DNS connection.</p> <p></p>"},{"location":"proxmox-cluster/solutions/","title":"Solutions","text":"<p>Here is a compiled list of solutions to the many problems I had to solve for this project. Maybe this can be of use for other people!</p>"},{"location":"proxmox-cluster/solutions/PVE%20WiFi%20Enable/","title":"PVE WiFi Enable","text":""},{"location":"proxmox-cluster/solutions/PVE%20WiFi%20Enable/#enabling-wifi-on-proxmox-ve-911","title":"Enabling WiFi on Proxmox VE 9.1.1","text":"<p>I followed a guide made by Ikhlash Rakhmanta to enable the wireless network on each of my servers. While this worked well for the Hypervisor host, the system was not yet configured to provide routing between this interface and any VMs or Containers.</p>"},{"location":"journal/archive/2026/","title":"2026","text":""},{"location":"journal/archive/2025/","title":"2025","text":""},{"location":"journal/category/nas/","title":"NAS","text":""},{"location":"journal/category/pve-cluster/","title":"PVE Cluster","text":""}]}